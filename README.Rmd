---
title: "Your Guide to DataFest"
author: "Chris Nobblitt"
date: "March 28, 2017"
output: 
  html_document:
    toc: TRUE
    toc_depth: 2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##### Adapted work from [tyson-ni](https://github.com/tyson-ni/DataFest-Guide) on GitHub



## **Stategies For Large Datasets in R:**

> R stores its objects in RAM...So, close Chrome!

**Buy More RAM**
 * $$$
 

**Limit RAM Usage** There's not a lot of it...

 * Kill programs other than R (especially Chrome)
 * Read a subset of original data
 * Remove unnecessary columns


*** 


## Download The Data

Click *Download Sample Data* and save to the *data* folder in your working directory (where your R code is)

* [Download](https://www.ookla.com/speedtest-intelligence)


_**Remember**_: Whenever you have questions about how to use code or what to put in a function, type this in console:

*`?feature-I-am-confused-about`*


***


## Importing Data


### readr

Imports and exports data *much* faster than default R.

* [manual](https://github.com/hadley/readr/blob/master/README.md)

``` {r, eval=FALSE}
# Run this if you havn't installed readr
install.packages("readr")
```

### Subset the Data!

Start with a *small* sample (e.g. 500 - 5000 observations) of the original dataset. Read increasingly more observations in as you grow comfortable AND have working code.

Readr also has a nifty **progress bar**!

``` {r, warning=FALSE}
library(readr)
android <- read_csv("data/android_data_sample.csv", n_max=500)
```

### Exclude unnecessary columns
If there is information not useful for anything, take it out!

``` {r}
head(android$test_id)
```

Hmm... we don't really need `test_id` for our analysis.
Remove it! (The FIRST column)

```{r}
android <- android[, -1] # Keep everything but the 1st column
names(android)
```

***

  
## Cleaning and Analyzing Data

You'll spend the most time cleaning data. Don't give up. Patience pays off.

### Look at your data

Staring at a giant table isn't effective. Summaries and Visuals are the best way to digest what you have.

* data structure - `str(dataset)`
* data summary - `summary(dataset)`
* Dplyr's summary - `glimpse(dataset)`
* look at a few rows - `head(data)` and `tail(data)`
* histograms - `hist(col)`
* get fancy with correlations `cor(data)`

Once you explore and start noticing things about the data, you can start manipulating it.


### dplyr

The single most powerful library in R. Summarize, filter, remove, transform and create new data... You'll save lots of time and trouble if you use this instead of default R.

* [best intro tutorial](https://cran.rstudio.com/web/packages/dplyr/vignettes/introduction.html)

* [PDF cheatsheet](https://github.com/rstudio/cheatsheets/raw/master/source/pdfs/data-transformation-cheatsheet.pdf)

* [another tutorial](http://genomicsclass.github.io/book/pages/dplyr_tutorial.html)

* [yet another tutorial](http://www.r-bloggers.com/data-manipulation-with-dplyr/)

#### Filter rows with `Filter`

> filter(data, logicalExpression1, logicalExpression2, ...)

Use `filter` to easily highlight certain observations, or to remove observations you do not want in the data (e.g. outliers). You can include as many logical expressions as you want, just make sure you have enough data leftover for analysis!

For example, say we wanted to look at Sprint customers with Samsung phones:

```{r}
library(dplyr, quietly = TRUE, warn.conflicts = FALSE) # I made this load quietly
tbl_df(filter(android, isp_name == "Sprint PCS", brand == "samsung"))
```



#### Arrange rows with `arrange`

> arrange(data, col1, col2, ...)

Use `arrange` to sort the rows in a table in ascending order by the specified column. Additional columns are tie breakers when two or more observations have the same value. Also, wrap `desc()` around the column name to arrange in descending order.

#### Select columns with `select`

> select(data, col1, col2, ...)

Like the SQL select, `select` is used to keep only variables of interest from your table. Say we only wanted location information about each observation.

``` {r}
select(android, android_device_id, gmaps_name, gmaps_country, gmaps_region, gmaps_subregion)

#You can also keep everything except a column!
select(android, -android_fingerprint)
```

#### Summarise with `summarize`

> summarise(data, custom = function(col1), ...)

Produce summaries of your data by replacing `function`above with summary functions, including `mean()`, `n()`, etc.

```{r}
summarise(android, avgUp = mean(upload_kbps, na.rm = T),
          avgDown = mean(download_kbps, na.rm = T))
```


#### Pipe Operators
*%>%* Saves a lot of memory, and is much more readable

```{r, eval=FALSE}
 dataset <- read_csv(android) %>%
          dplyr::group_by(device) %>%
          dplyr::summarise(avgSpeed = download_kbps) %>%
          dplyr::arrange(avgSpeed)
```
             
This is the same code as:

```{r, eval=FALSE}
data set <- arrange(summarise(group_by(read_csv(android), device), avgSpeed = download_kbps), avgSpeed))
```


### tidyr

Working with 'tidy' data is the easiest way for quick analysis.

* [tutorial](http://data.library.virginia.edu/a-tidyr-tutorial/)

Tidy data is usually:

* One observation per row
* Columns are not values, they are variables
* Individual cells of the table are values for each column


### stringr

Great for working with strings

* [tutorial](https://cran.r-project.org/web/packages/stringr/vignettes/stringr.html)

### lubridate

* A quick way to format variables that store time and dates
* [tutorial](https://cran.r-project.org/web/packages/lubridate/vignettes/lubridate.html)

### regular expression (regex)

> Let's say you want to extract the height and width dimensions in an artworks dataset. The original variable stores the info like this mess: `19 1/8 x 66 1/2"(48.6 x 168.9cm)`

> We only care about `48.6` and `168.9`. We do this through REGEX. 

> This is the regex pattern to get `48.6` : `"\\(([:digit:]+\\.*[:digit:]*)\\s[x×]"`

> This is the regex pattern to get `168.9` : `"[x×]\\s([:digit:]+\\.*[:digit:]*)\\scm"`

* Great website to learn regex and practice live! 
* [link](http://www.regexr.com/)


***

### pandas in Python

For Python coders

* [10 minute tutorial](http://pandas.pydata.org/pandas-docs/version/0.15.2/10min.html#min)

***


### Quartz's guide to bad data

* A collection of common data recording problems and ways to fix them
* [link](https://github.com/Quartz/bad-data-guide)

***

## Visualization

Wanna win the Best Visualization prize?

### ggplot2

Best library for creating graphics in R

### Tableau

A software for creating graphics using drag and drop interface. No code necessary. Play around with this if you're stuck! Tableau Public is free.

* [download](https://public.tableau.com/s/download)
* [tutorial](http://dh101.humanities.ucla.edu/?page_id=163)
* [gallery](https://public.tableau.com/s/gallery)

### D3

Behind some of the most impressive interactive visualizations. Great for creating original graphics that perfectly matches your quirky mental model of what data looks like.

* [great series of tutorials](http://alignedleft.com/tutorials/d3/)
* [demos with code included](http://bl.ocks.org/mbostock)
* [reference manual](https://github.com/mbostock/d3/wiki/API-Reference)

***

## Machine Learning

> Everybody is talking about it. From my experience, DataFest judges want you to *explain why something is the case* rather than predict what might happen. Machine learning usually excels at prediction but not explanation. But definitely take this opportunity to practice ML.

### A curated list of resources

This person did an amazing job covering topics from classification and regression to deep learning and ensembles 

* [link](https://github.com/ujjwalkarn/Machine-Learning-Tutorials)

## Practice!
Use these datasets as practice - a truly awesome collection of many kinds of public datasets, covering weather, economics, health and many more

* [link](https://github.com/caesar0301/awesome-public-datasets)








